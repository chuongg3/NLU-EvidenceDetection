{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-Attention Siamese Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 22:40:30.636023: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-03 22:40:30.731416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-03 22:40:30.773434: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-03 22:40:30.792441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-03 22:40:30.864177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-03 22:40:31.630556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file_path = './training_data/training_data/ED/dev.csv'\n",
    "input_file_path = './test_data/ED/test.csv'\n",
    "model_file_path = 'CoAttentionSiameseDeepLearning.keras'\n",
    "\n",
    "threshold = 0.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dev data\n",
    "data_path = './training_data/training_data/ED/dev.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instances of [ref] from the text\n",
    "import re\n",
    "\n",
    "remove_ref = lambda x: re.sub(r'\\[ref\\]|\\[ref|ref\\]', '', x)\n",
    "\n",
    "data['Claim'] = data['Claim'].apply(remove_ref)\n",
    "data['Evidence'] = data['Evidence'].apply(remove_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuongg3/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the Encoder\n",
    "encoder_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# encoder_model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "# Encode the Training Set\n",
    "claim_embeddings = encoder_model.encode(data['Claim'])\n",
    "evidence_embeddings = encoder_model.encode(data['Evidence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MultiHead Co-Attention layer\n",
    "class MultiHeadCoAttention(layers.Layer):\n",
    "    def __init__(self, attention_dim=128, num_heads=4, dropout_rate=0.1, **kwargs):\n",
    "        super(MultiHeadCoAttention, self).__init__(**kwargs)\n",
    "        self.attention_dim = attention_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = attention_dim // num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Ensure the attention dimension is divisible by the number of heads\n",
    "        assert attention_dim % num_heads == 0, \"Attention dimension must be divisible by number of heads\"\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Ensure we have two inputs\n",
    "        assert isinstance(input_shape, list) and len(input_shape) == 2\n",
    "        \n",
    "        self.claim_dim = input_shape[0][-1]\n",
    "        self.evidence_dim = input_shape[1][-1]\n",
    "        \n",
    "        # Claim projections\n",
    "        self.claim_projection = layers.Dense(self.attention_dim * 3, use_bias=False)\n",
    "        \n",
    "        # Evidence projections\n",
    "        self.evidence_projection = layers.Dense(self.attention_dim * 3, use_bias=False)\n",
    "        \n",
    "        # Output projections\n",
    "        self.claim_output_projection = layers.Dense(self.claim_dim, use_bias=False)\n",
    "        self.evidence_output_projection = layers.Dense(self.evidence_dim, use_bias=False)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "        \n",
    "        super(MultiHeadCoAttention, self).build(input_shape)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim) with fixed batch size\"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Reshape with fixed dimensions where possible\n",
    "        x = tf.reshape(x, [batch_size, seq_len, self.num_heads, self.head_dim])\n",
    "        return tf.transpose(x, [0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Unpack inputs\n",
    "        claim, evidence = inputs\n",
    "\n",
    "        # Get projection for claim (Q, K, V)\n",
    "        claim_proj = self.claim_projection(claim)\n",
    "\n",
    "        # Split claim projection into query, key, value\n",
    "        claim_proj_split = tf.split(claim_proj, 3, axis=-1)\n",
    "        claim_query, claim_key, claim_value = claim_proj_split\n",
    "\n",
    "        # Get projection for evidence (Q, K, V)\n",
    "        evidence_proj = self.evidence_projection(evidence)\n",
    "\n",
    "        # Split evidence projection into query, key, value\n",
    "        evidence_proj_split = tf.split(evidence_proj, 3, axis=-1)\n",
    "        evidence_query, evidence_key, evidence_value = evidence_proj_split\n",
    "\n",
    "        # Split heads\n",
    "        claim_query_heads = self.split_heads(claim_query)\n",
    "        claim_key_heads = self.split_heads(claim_key)\n",
    "        claim_value_heads = self.split_heads(claim_value)\n",
    "\n",
    "        evidence_query_heads = self.split_heads(evidence_query)\n",
    "        evidence_key_heads = self.split_heads(evidence_key)\n",
    "        evidence_value_heads = self.split_heads(evidence_value)\n",
    "\n",
    "        # Claim attends to evidence with fixed scaling factor\n",
    "        claim_evidence_scores = tf.matmul(claim_query_heads, evidence_key_heads, transpose_b=True)\n",
    "        claim_evidence_scores = claim_evidence_scores / tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "\n",
    "        claim_evidence_attention = tf.nn.softmax(claim_evidence_scores, axis=-1)\n",
    "        claim_evidence_attention = self.dropout(claim_evidence_attention, training=training)\n",
    "\n",
    "        claim_context = tf.matmul(claim_evidence_attention, evidence_value_heads)\n",
    "\n",
    "        # Evidence attends to claim with fixed scaling factor\n",
    "        evidence_claim_scores = tf.matmul(evidence_query_heads, claim_key_heads, transpose_b=True)\n",
    "        evidence_claim_scores = evidence_claim_scores / tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "\n",
    "        evidence_claim_attention = tf.nn.softmax(evidence_claim_scores, axis=-1)\n",
    "        evidence_claim_attention = self.dropout(evidence_claim_attention, training=training)\n",
    "\n",
    "        evidence_context = tf.matmul(evidence_claim_attention, claim_value_heads)\n",
    "\n",
    "        # Combine heads and transpose back\n",
    "        claim_context = tf.transpose(claim_context, [0, 2, 1, 3])\n",
    "        evidence_context = tf.transpose(evidence_context, [0, 2, 1, 3])\n",
    "\n",
    "        # Instead of reshaping to force seq_len=1, perform average pooling over the sequence dimension:\n",
    "        claim_context = tf.reduce_mean(claim_context, axis=1, keepdims=True)\n",
    "        evidence_context = tf.reduce_mean(evidence_context, axis=1, keepdims=True)\n",
    "\n",
    "        # Reshape with fixed dimensions where possible\n",
    "        batch_size = tf.shape(claim)[0]\n",
    "        claim_context = tf.reshape(claim_context, [batch_size, 1, self.attention_dim])\n",
    "        evidence_context = tf.reshape(evidence_context, [batch_size, 1, self.attention_dim])\n",
    "\n",
    "        # Project back to original dimensions using Dense layers\n",
    "        claim_output = self.claim_output_projection(claim_context)\n",
    "        evidence_output = self.evidence_output_projection(evidence_context)\n",
    "\n",
    "        return claim_output, evidence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743716444.176623    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1743716444.188114    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1743716444.188739    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1743716444.192117    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1743716444.192639    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1743716444.193070    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1743716444.193651    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1743716444.194088    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1743716444.194556    6020 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-03 22:40:44.195205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1899 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Load the model with custom layer\n",
    "model = tf.keras.models.load_model(\n",
    "    model_file_path, \n",
    "    custom_objects={'MultiHeadCoAttention': MultiHeadCoAttention}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get True and Predicted Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict([claim_embeddings, evidence_embeddings])\n",
    "y_true = data['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threhold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.34000000000000014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Threshold Tuning\n",
    "thresholds = np.arange(0.2, 0.7, 0.01)\n",
    "scores = [f1_score(y_true, (results > t).astype(int), average='macro') for t in thresholds]\n",
    "bestThreshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best Threshold: {bestThreshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.8812868147 0.9076061596 0.8942528736      4286\n",
      "           1  0.7380952381 0.6804878049 0.7081218274      1640\n",
      "\n",
      "    accuracy                      0.8447519406      5926\n",
      "   macro avg  0.8096910264 0.7940469822 0.8011873505      5926\n",
      "weighted avg  0.8416590412 0.8447519406 0.8427417504      5926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Apply the best threshold to the predictions\n",
    "y_pred_binary = (results > bestThreshold).astype(int)\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(y_true=y_true, y_pred=y_pred_binary, digits=10)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient: 0.6035352905480197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
    "print(f\"Matthews Correlation Coefficient: {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
