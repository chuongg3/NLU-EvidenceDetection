{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffa72d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (0.29.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from seaborn) (2.2.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\para_\\desktop\\code\\uni\\nlu\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn matplotlib huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8b54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef, classification_report, precision_recall_curve\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140b79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for dual embedding model that ensures all tensors are on CPU.\n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with preprocessed features.\n",
    "\n",
    "        Args:\n",
    "            features: Dictionary of feature tensors including input_ids,\n",
    "                      attention_mask, sbert_embeddings, and labels\n",
    "        \"\"\"\n",
    "        self.input_ids = features[\"input_ids\"]\n",
    "        self.attention_mask = features[\"attention_mask\"]\n",
    "        self.sbert_embeddings = features[\"sbert_embeddings\"]\n",
    "        self.labels = features[\"labels\"]\n",
    "\n",
    "        # Ensure all tensors are on CPU\n",
    "        if self.input_ids.is_cuda:\n",
    "            self.input_ids = self.input_ids.cpu()\n",
    "        if self.attention_mask.is_cuda:\n",
    "            self.attention_mask = self.attention_mask.cpu()\n",
    "        if self.sbert_embeddings.is_cuda:\n",
    "            self.sbert_embeddings = self.sbert_embeddings.cpu()\n",
    "        if self.labels.is_cuda:\n",
    "            self.labels = self.labels.cpu()\n",
    "\n",
    "        # Validate tensor shapes\n",
    "        assert len(self.input_ids) == len(self.attention_mask) == len(self.sbert_embeddings) == len(self.labels), \\\n",
    "            \"All feature tensors must have the same first dimension\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single example from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: Index to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of tensors for the given index\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"sbert_embeddings\": self.sbert_embeddings[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9420c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dual_embedding_features(df, modernbert_tokenizer, sbert_model, max_length=512, sbert_batch_size=64):\n",
    "    \"\"\"\n",
    "    Prepare features for the dual embedding model, ensuring all tensors remain on CPU.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'Claim', 'Evidence', and label columns\n",
    "        modernbert_tokenizer: ModernBERT tokenizer\n",
    "        sbert_model: Sentence-BERT model\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "        sbert_batch_size: Batch size for SBERT encoding\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of feature tensors with input_ids, attention_mask, sbert_embeddings, and labels\n",
    "    \"\"\"\n",
    "    # Keep track of original SBERT device\n",
    "    original_device = next(sbert_model.parameters()).device\n",
    "    logger.info(f\"Original SBERT device: {original_device}\")\n",
    "\n",
    "    # Prepare inputs\n",
    "    texts_claim = df[\"Claim\"].tolist()\n",
    "    texts_evidence = df[\"Evidence\"].tolist()\n",
    "\n",
    "    # ModernBERT tokenization - keep on CPU\n",
    "    logger.info(\"Tokenizing inputs for ModernBERT...\")\n",
    "    modernbert_features = modernbert_tokenizer(\n",
    "        texts_claim,\n",
    "        texts_evidence,\n",
    "        padding=True,\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Compute SBERT embeddings on GPU, then move back to CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Computing SBERT embeddings on: {device}\")\n",
    "\n",
    "    # Temporarily move SBERT to computation device\n",
    "    sbert_model = sbert_model.to(device)\n",
    "\n",
    "    # Compute claim embeddings\n",
    "    logger.info(\"Computing SBERT embeddings for claims (batched)...\")\n",
    "    claim_embeddings = sbert_model.encode(\n",
    "        texts_claim,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=sbert_batch_size,\n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Move claim embeddings to CPU immediately\n",
    "    claim_embeddings = claim_embeddings.cpu()\n",
    "\n",
    "    # Compute evidence embeddings\n",
    "    logger.info(\"Computing SBERT embeddings for evidence (batched)...\")\n",
    "    evidence_embeddings = sbert_model.encode(\n",
    "        texts_evidence,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=sbert_batch_size,\n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Move evidence embeddings to CPU immediately\n",
    "    evidence_embeddings = evidence_embeddings.cpu()\n",
    "\n",
    "    # Return SBERT to original device\n",
    "    sbert_model = sbert_model.to(original_device)\n",
    "\n",
    "    # Combine claim and evidence embeddings on CPU\n",
    "    logger.info(\"Combining embeddings...\")\n",
    "    combined_embeddings = []\n",
    "    for claim_emb, evid_emb in tqdm(zip(claim_embeddings, evidence_embeddings),\n",
    "                                  total=len(claim_embeddings),\n",
    "                                  desc=\"Combining embeddings\"):\n",
    "        # Use average of the claim and evidence embeddings\n",
    "        combined_emb = (claim_emb + evid_emb) / 2\n",
    "        combined_embeddings.append(combined_emb)\n",
    "\n",
    "    sbert_embeddings = torch.stack(combined_embeddings)\n",
    "\n",
    "    # Prepare labels\n",
    "    if \"label\" in df.columns:\n",
    "        label_col = \"label\"\n",
    "    elif \"labels\" in df.columns:\n",
    "        label_col = \"labels\"\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame must contain 'label' or 'labels' column\")\n",
    "\n",
    "    # Keep labels on CPU\n",
    "    labels = torch.tensor(df[label_col].values, dtype=torch.float)\n",
    "\n",
    "    # Final verification that all tensors are on CPU\n",
    "    logger.info(\"Verifying all tensors are on CPU...\")\n",
    "    for key, tensor in modernbert_features.items():\n",
    "        if tensor.is_cuda:\n",
    "            logger.warning(f\"{key} is on CUDA, moving to CPU\")\n",
    "            modernbert_features[key] = tensor.cpu()\n",
    "\n",
    "    if sbert_embeddings.is_cuda:\n",
    "        logger.warning(\"sbert_embeddings is on CUDA, moving to CPU\")\n",
    "        sbert_embeddings = sbert_embeddings.cpu()\n",
    "\n",
    "    if labels.is_cuda:\n",
    "        logger.warning(\"labels is on CUDA, moving to CPU\")\n",
    "        labels = labels.cpu()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": modernbert_features[\"input_ids\"],\n",
    "        \"attention_mask\": modernbert_features[\"attention_mask\"],\n",
    "        \"sbert_embeddings\": sbert_embeddings,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d469bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DualEmbeddingModel class again for loading\n",
    "class DualEmbeddingModel(nn.Module):\n",
    "    def __init__(self, modernbert_model, sbert_dim=384, hidden_size=768, dropout_rate=0.1):\n",
    "        super(DualEmbeddingModel, self).__init__()\n",
    "        self.modernbert = modernbert_model\n",
    "        \n",
    "        # Get embedding dimensions\n",
    "        self.modernbert_dim = modernbert_model.config.hidden_size  # 768 for ModernBERT-base\n",
    "        self.sbert_dim = sbert_dim\n",
    "        \n",
    "        # Classifier with variable hidden size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.modernbert_dim + self.sbert_dim, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, sbert_embeddings, labels=None):\n",
    "        # Ensure inputs are on the same device as the model parameters\n",
    "        device = self.device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        sbert_embeddings = sbert_embeddings.to(device)\n",
    "        \n",
    "        # Get ModernBERT embedding for [CLS] token\n",
    "        modernbert_outputs = self.modernbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        modernbert_embedding = modernbert_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        combined_embedding = torch.cat([modernbert_embedding, sbert_embeddings], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(combined_embedding).squeeze(-1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df5137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dual_embedding_model_from_hub(repo_id, device=None):\n",
    "    \"\"\"\n",
    "    Load a DualEmbeddingModel from Hugging Face Hub.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Hugging Face repository ID (e.g., \"username/model-name\")\n",
    "        device: Device to load the model to\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load tokenizer from Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "    \n",
    "    # Load SBERT model\n",
    "    sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Set up quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load base ModernBERT model\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        \"answerdotai/ModernBERT-base\",\n",
    "        quantization_config=quant_config,\n",
    "        device_map=device\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapters\n",
    "    peft_model = PeftModel.from_pretrained(base_model, repo_id, inference_mode=True)\n",
    "    \n",
    "    # Create DualEmbeddingModel\n",
    "    model = DualEmbeddingModel(peft_model)\n",
    "    \n",
    "    # Load classifier weights using huggingface_hub\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    \n",
    "    # Download classifier weights file\n",
    "    classifier_path = hf_hub_download(repo_id=repo_id, filename=\"classifier_weights.pt\")\n",
    "    classifier_weights = torch.load(classifier_path, map_location=device)\n",
    "    model.classifier.load_state_dict(classifier_weights)\n",
    "    \n",
    "    # Load optimal threshold\n",
    "    threshold_path = hf_hub_download(repo_id=repo_id, filename=\"optimal_threshold.txt\")\n",
    "    with open(threshold_path, \"r\") as f:\n",
    "        threshold = float(f.read().strip())\n",
    "    \n",
    "    model.eval()\n",
    "    return model, tokenizer, sbert_model, threshold\n",
    "\n",
    "\n",
    "def load_dual_embedding_model_local(model_dir, device=None):\n",
    "    \"\"\"\n",
    "    Load a DualEmbeddingModel from a local directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Path to the local directory containing the model files\n",
    "        device: Device to load the model to (default: use CUDA if available)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Loading model from {model_dir} to {device}\")\n",
    "    \n",
    "    # Load tokenizer from local directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    # Load SBERT model\n",
    "    sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Set up quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load base ModernBERT model\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        \"answerdotai/ModernBERT-base\",\n",
    "        quantization_config=quant_config,\n",
    "        device_map=device\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapters from local directory\n",
    "    peft_model = PeftModel.from_pretrained(base_model, model_dir, inference_mode=True)\n",
    "    \n",
    "    # Create DualEmbeddingModel\n",
    "    model = DualEmbeddingModel(peft_model)\n",
    "    \n",
    "    # Load classifier weights from local file\n",
    "    classifier_weights_path = os.path.join(model_dir, \"classifier_weights.pt\")\n",
    "    if os.path.exists(classifier_weights_path):\n",
    "        classifier_weights = torch.load(classifier_weights_path, map_location=device)\n",
    "        model.classifier.load_state_dict(classifier_weights)\n",
    "        print(f\"Loaded classifier weights from {classifier_weights_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: Classifier weights file not found at {classifier_weights_path}\")\n",
    "    \n",
    "    # Load optimal threshold\n",
    "    threshold_path = os.path.join(model_dir, \"optimal_threshold.txt\")\n",
    "    threshold = 0.5  # Default threshold\n",
    "    if os.path.exists(threshold_path):\n",
    "        with open(threshold_path, \"r\") as f:\n",
    "            threshold = float(f.read().strip())\n",
    "        print(f\"Loaded optimal threshold: {threshold}\")\n",
    "    else:\n",
    "        print(f\"Warning: Threshold file not found at {threshold_path}, using default: {threshold}\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model, tokenizer, sbert_model, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5f42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for classification based on F1 score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): True labels.\n",
    "        y_pred (np.ndarray): Predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "        float: Optimal threshold.\n",
    "    \"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "\n",
    "    f1_scores = np.divide(\n",
    "        2 * (precisions * recalls),\n",
    "        (precisions + recalls),\n",
    "        out=np.zeros_like(precisions),\n",
    "        where=(precisions + recalls) > 0\n",
    "    )\n",
    "\n",
    "    best_idx = np.argmax(f1_scores[:-1])\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "\n",
    "    logger.info(f\"Best threshold: {best_threshold:.4f} with F1: {best_f1:.4f}\")\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49af4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_threshold(model, dataloader, device=None):\n",
    "    \"\"\"\n",
    "    Calculate the best threshold for the model using the validation set,\n",
    "    converting logits to probabilities with sigmoid.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        dataloader: DataLoader for the validation set.\n",
    "        device: Device to run the model on. If None, will use CUDA if available.\n",
    "\n",
    "    Returns:\n",
    "        float: Best threshold based on F1 score.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "    import logging\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_labels = []\n",
    "    all_probs = []  # We'll store probabilities, not logits\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Calculating best threshold\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            sbert_embeddings = batch[\"sbert_embeddings\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Get logits from model\n",
    "            logits = model(input_ids, attention_mask, sbert_embeddings)\n",
    "            \n",
    "            # Convert logits to probabilities using sigmoid\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all probabilities and labels\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # Find the optimal threshold based on F1 score\n",
    "    best_threshold = find_optimal_threshold(all_labels, all_probs)\n",
    "\n",
    "    logger.info(f\"Best threshold: {best_threshold:.4f}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b118959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_dir, test_df, batch_size=16, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate the saved DualEmbeddingModel on a test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing the saved model and tokenizer.\n",
    "        test_df (pd.DataFrame): Test dataframe with 'Claim', 'Evidence', and 'label' columns.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        device (str, optional): Device to load the model to.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef\n",
    "    \n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load the model\n",
    "    model, tokenizer, sbert_model, threshold = load_dual_embedding_model_local(model_dir, device)\n",
    "    \n",
    "    # Explicitly move the entire model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Make sure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare the test dataset\n",
    "    test_features = prepare_dual_embedding_features(test_df, tokenizer, sbert_model)\n",
    "    test_dataset = DualEmbeddingDataset(test_features)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Perform evaluation\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\", disable=False):\n",
    "            # Explicitly move batch data to the same device as the model\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            sbert_embeddings = batch[\"sbert_embeddings\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, sbert_embeddings)\n",
    "            \n",
    "            # Move results back to CPU for metric calculation\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    # Apply sigmoid to convert logits to probabilities\n",
    "    probabilities = torch.sigmoid(torch.tensor(all_logits)).numpy()\n",
    "    \n",
    "    # Make predictions using the optimal threshold\n",
    "    predictions = (probabilities > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, predictions, average='macro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_labels, predictions, average='weighted')\n",
    "    mcc = matthews_corrcoef(all_labels, predictions)\n",
    "\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, predictions, target_names=[\"0\", \"1\"]))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    cm = confusion_matrix(all_labels, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='crest', xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"])\n",
    "    plt.title(\"Confusion Matrix for ModernBERT + SBERT\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"Evaluation Results (threshold={threshold:.4f}):\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision (macro): {precision:.4f}\")\n",
    "    print(f\"  Recall (macro): {recall:.4f}\")\n",
    "    print(f\"  F1 Score (macro): {f1:.4f}\")\n",
    "    print(f\"  Precision (weighted): {precision_macro:.4f}\")\n",
    "    print(f\"  Recall (weighted): {recall_macro:.4f}\")\n",
    "    print(f\"  F1 Score (weighted): {f1_macro:.4f}\")\n",
    "    print(f\"  MCC: {mcc:.4f}\")\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        \"threshold\": threshold,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"precision_w\": precision_macro,\n",
    "        \"recall_w\": recall_macro,\n",
    "        \"f1_w\": f1_macro,\n",
    "        \"mcc\": mcc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d489aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model_dir, test_df, batch_size=64, device=None):\n",
    "    \"\"\"\n",
    "    Use the saved DualEmbeddingModel to make predictions on a batch of claim-evidence pairs.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing the saved model and tokenizer.\n",
    "        test_df (pd.DataFrame): Test dataframe with 'Claim' and 'Evidence' columns.\n",
    "        batch_size (int): Batch size for prediction.\n",
    "        device (str, optional): Device to load the model to.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Original dataframe with 'prediction' and 'probability' columns added.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model, tokenizer, sbert_model, threshold = load_dual_embedding_model_local(model_dir, device=device)\n",
    "\n",
    "    # Explicitly move the entire model to the specified device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Make sure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare the test dataset\n",
    "    test_features = prepare_dual_embedding_features(test_df, tokenizer, sbert_model)\n",
    "    test_dataset = DualEmbeddingDataset(test_features)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Perform predictions\n",
    "    all_logits = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\", disable=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            sbert_embeddings = batch[\"sbert_embeddings\"].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, sbert_embeddings)\n",
    "            all_logits.append(logits.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    \n",
    "    # Convert logits to probabilities with sigmoid\n",
    "    probabilities = torch.sigmoid(torch.tensor(all_logits)).numpy()\n",
    "    \n",
    "    # Make predictions using the optimal threshold\n",
    "    predictions = (probabilities > threshold).astype(int)\n",
    "    \n",
    "    # Add predictions and probabilities to the dataframe\n",
    "    result_df = test_df.copy()\n",
    "    result_df[\"prediction\"] = predictions\n",
    "    result_df[\"probability\"] = probabilities\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"training_data/ED\"\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(DATASET_DIR, \"dev.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82680cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from final_model/best_model to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:Original SBERT device: cuda:0\n",
      "INFO:__main__:Tokenizing inputs for ModernBERT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifier weights from final_model/best_model\\classifier_weights.pt\n",
      "Loaded optimal threshold: 0.6002469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Computing SBERT embeddings on: cuda\n",
      "INFO:__main__:Computing SBERT embeddings for claims (batched)...\n",
      "Batches: 100%|██████████| 93/93 [00:01<00:00, 91.32it/s] \n",
      "INFO:__main__:Computing SBERT embeddings for evidence (batched)...\n",
      "Batches: 100%|██████████| 93/93 [00:03<00:00, 26.13it/s]\n",
      "INFO:__main__:Combining embeddings...\n",
      "Combining embeddings: 100%|██████████| 5926/5926 [00:00<00:00, 22767.92it/s]\n",
      "INFO:__main__:Verifying all tensors are on CPU...\n",
      "Evaluating: 100%|██████████| 93/93 [02:11<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92      4286\n",
      "           1       0.80      0.75      0.77      1640\n",
      "\n",
      "    accuracy                           0.88      5926\n",
      "   macro avg       0.85      0.84      0.85      5926\n",
      "weighted avg       0.88      0.88      0.88      5926\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results (threshold=0.6002):\n",
      "  Accuracy: 0.8783\n",
      "  Precision (macro): 0.8518\n",
      "  Recall (macro): 0.8397\n",
      "  F1 Score (macro): 0.8454\n",
      "  Precision (weighted): 0.8767\n",
      "  Recall (weighted): 0.8783\n",
      "  F1 Score (weighted): 0.8773\n",
      "  MCC: 0.6914\n"
     ]
    }
   ],
   "source": [
    "# Load the model directory\n",
    "model_dir = \"ddosdub/DualEncoderModernBERT\"\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_model(model_dir, test_df, batch_size=64, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23748aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from final_model/best_model to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:__main__:Original SBERT device: cuda:0\n",
      "INFO:__main__:Tokenizing inputs for ModernBERT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classifier weights from final_model/best_model\\classifier_weights.pt\n",
      "Loaded optimal threshold: 0.6002469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Computing SBERT embeddings on: cuda\n",
      "INFO:__main__:Computing SBERT embeddings for claims (batched)...\n",
      "Batches: 100%|██████████| 93/93 [00:01<00:00, 76.39it/s]\n",
      "INFO:__main__:Computing SBERT embeddings for evidence (batched)...\n",
      "Batches: 100%|██████████| 93/93 [00:03<00:00, 24.58it/s]\n",
      "INFO:__main__:Combining embeddings...\n",
      "Combining embeddings: 100%|██████████| 5926/5926 [00:00<00:00, 62275.78it/s]\n",
      "INFO:__main__:Verifying all tensors are on CPU...\n",
      "Predicting: 100%|██████████| 93/93 [02:13<00:00,  1.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Democracy should be done away with.</td>\n",
       "      <td>Amartya Sen, an Indian economist and Nobel lau...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polygamy should be made legal.</td>\n",
       "      <td>The Supreme Court's unanimous decision in Reyn...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hunting should be banned</td>\n",
       "      <td>In total it is estimated that Ceauşescu receiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Television should be given up.</td>\n",
       "      <td>Barbera mentioned that they had to either adju...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abortions ought to be prohibited.</td>\n",
       "      <td>According to a poll conducted by Angus Reid St...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Claim  \\\n",
       "0  Democracy should be done away with.   \n",
       "1       Polygamy should be made legal.   \n",
       "2             Hunting should be banned   \n",
       "3       Television should be given up.   \n",
       "4    Abortions ought to be prohibited.   \n",
       "\n",
       "                                            Evidence  label  prediction  \\\n",
       "0  Amartya Sen, an Indian economist and Nobel lau...      0           0   \n",
       "1  The Supreme Court's unanimous decision in Reyn...      1           1   \n",
       "2  In total it is estimated that Ceauşescu receiv...      0           0   \n",
       "3  Barbera mentioned that they had to either adju...      0           0   \n",
       "4  According to a poll conducted by Angus Reid St...      1           1   \n",
       "\n",
       "   probability  \n",
       "0     0.014555  \n",
       "1     0.980272  \n",
       "2     0.003314  \n",
       "3     0.003216  \n",
       "4     0.987796  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df = predict_model(model_dir, test_df, batch_size=64, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3645477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction column from the predict_df\n",
    "predictions = predict_df[\"prediction\"]\n",
    "\n",
    "# convert to csv\n",
    "predictions.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "def create_file_for_submission(predictions: pd.DataFrame, ext: str = \"predict\"):\n",
    "    predictions.to_csv(f\"predictions.csv.{ext}\", index=False)\n",
    "\n",
    "    # zip the file\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"predictions.zip\", \"w\") as zf:\n",
    "        zf.write(f\"predictions.csv.{ext}\")\n",
    "\n",
    "create_file_for_submission(pd.read_csv(\"predictions.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
