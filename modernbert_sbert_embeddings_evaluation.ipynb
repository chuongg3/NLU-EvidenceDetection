{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6dc6ee",
   "metadata": {},
   "source": [
    "# ModernBERT + SBERT Dual Embedding Model Evaluation\n",
    "\n",
    "This notebook evaluates a dual embedding model that combines ModernBERT and Sentence-BERT (SBERT) for the task of evidence detection. The model uses a combination of contextualized embeddings from ModernBERT and sentence embeddings from SBERT to predict whether a piece of evidence supports a given claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b45986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:09.535485Z",
     "iopub.status.busy": "2025-04-05T15:45:09.535193Z",
     "iopub.status.idle": "2025-04-05T15:45:12.877165Z",
     "shell.execute_reply": "2025-04-05T15:45:12.876089Z",
     "shell.execute_reply.started": "2025-04-05T15:45:09.535461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71fae744-7969-46f5-b769-a86452dc3cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:12.879123Z",
     "iopub.status.busy": "2025-04-05T15:45:12.878785Z",
     "iopub.status.idle": "2025-04-05T15:45:16.283825Z",
     "shell.execute_reply": "2025-04-05T15:45:16.282629Z",
     "shell.execute_reply.started": "2025-04-05T15:45:12.879096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy!=1.24.0,>=1.17->seaborn) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy!=1.24.0,>=1.17->seaborn) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy!=1.24.0,>=1.17->seaborn) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn matplotlib tqdm scikit-learn unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ffa72d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:16.286244Z",
     "iopub.status.busy": "2025-04-05T15:45:16.286007Z",
     "iopub.status.idle": "2025-04-05T15:45:20.412003Z",
     "shell.execute_reply": "2025-04-05T15:45:20.411097Z",
     "shell.execute_reply.started": "2025-04-05T15:45:16.286223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.50.3)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.4)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U huggingface_hub transformers bitsandbytes peft sentence-transformers pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c8b54e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.413634Z",
     "iopub.status.busy": "2025-04-05T15:45:20.413337Z",
     "iopub.status.idle": "2025-04-05T15:45:20.420757Z",
     "shell.execute_reply": "2025-04-05T15:45:20.419974Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.413605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef, classification_report, precision_recall_curve\n",
    "import re\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca1bcf",
   "metadata": {},
   "source": [
    "## Dataset and Model Architecture\n",
    "\n",
    "We define custom dataset classes to handle the dual embedding approach and the model architecture that combines ModernBERT and SBERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "140b79a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.421908Z",
     "iopub.status.busy": "2025-04-05T15:45:20.421638Z",
     "iopub.status.idle": "2025-04-05T15:45:20.435796Z",
     "shell.execute_reply": "2025-04-05T15:45:20.435216Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.421888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DualEmbeddingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for dual embedding model that ensures all tensors are on CPU.\n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with preprocessed features.\n",
    "\n",
    "        Args:\n",
    "            features: Dictionary of feature tensors including input_ids,\n",
    "                      attention_mask, sbert_embeddings, and labels\n",
    "        \"\"\"\n",
    "        self.input_ids = features[\"input_ids\"]\n",
    "        self.attention_mask = features[\"attention_mask\"]\n",
    "        self.sbert_embeddings = features[\"sbert_embeddings\"]\n",
    "        self.labels = features[\"labels\"] if \"labels\" in features else None\n",
    "\n",
    "        # Ensure all tensors are on CPU\n",
    "        if self.input_ids.is_cuda:\n",
    "            self.input_ids = self.input_ids.cpu()\n",
    "        if self.attention_mask.is_cuda:\n",
    "            self.attention_mask = self.attention_mask.cpu()\n",
    "        if self.sbert_embeddings.is_cuda:\n",
    "            self.sbert_embeddings = self.sbert_embeddings.cpu()\n",
    "        if self.labels is not None and self.labels.is_cuda:\n",
    "            self.labels = self.labels.cpu()\n",
    "\n",
    "        # Validate tensor shapes\n",
    "        assert len(self.input_ids) == len(self.attention_mask) == len(self.sbert_embeddings), \\\n",
    "            \"All feature tensors must have the same first dimension\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) if self.labels is not None else len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single example from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx: Index to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of tensors for the given index\n",
    "        \"\"\"\n",
    "        item = {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"sbert_embeddings\": self.sbert_embeddings[idx]\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfe6499a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.436810Z",
     "iopub.status.busy": "2025-04-05T15:45:20.436501Z",
     "iopub.status.idle": "2025-04-05T15:45:20.454542Z",
     "shell.execute_reply": "2025-04-05T15:45:20.453765Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.436782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the DualEmbeddingModel class again for loading\n",
    "class DualEmbeddingModel(nn.Module):\n",
    "    def __init__(self, modernbert_model, sbert_dim=384, hidden_size=768, dropout_rate=0):\n",
    "        super(DualEmbeddingModel, self).__init__()\n",
    "        self.modernbert = modernbert_model\n",
    "        \n",
    "        # Get embedding dimensions\n",
    "        self.modernbert_dim = modernbert_model.config.hidden_size  # 768 for ModernBERT-base\n",
    "        self.sbert_dim = sbert_dim\n",
    "        \n",
    "        # Classifier with variable hidden size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.modernbert_dim + self.sbert_dim, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, sbert_embeddings, labels=None):\n",
    "        # Ensure inputs are on the same device as the model parameters\n",
    "        device = self.device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        sbert_embeddings = sbert_embeddings.to(device)\n",
    "        \n",
    "        # Get ModernBERT embedding for [CLS] token\n",
    "        modernbert_outputs = self.modernbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        modernbert_embedding = modernbert_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        combined_embedding = torch.cat([modernbert_embedding, sbert_embeddings], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(combined_embedding).squeeze(-1)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6bc2a2",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "These functions prepare the text data for model input by cleaning and normalizing text and extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12ad7fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.455609Z",
     "iopub.status.busy": "2025-04-05T15:45:20.455416Z",
     "iopub.status.idle": "2025-04-05T15:45:20.471513Z",
     "shell.execute_reply": "2025-04-05T15:45:20.470888Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.455593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing reference tags and normalizing whitespace.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove reference tags\n",
    "    cleaned_text = re.sub(r\"\\[REF\\]|\\[REF|REF\\]\", \"\", text).strip()\n",
    "\n",
    "    # Normalize text\n",
    "    cleaned_text = unidecode.unidecode(cleaned_text)\n",
    "\n",
    "    punctuations = re.escape(string.punctuation)  # escape special characters like [ ] ( ) etc.\n",
    "\n",
    "    # Remove spaces between letter and punctuation\n",
    "    cleaned_text = re.sub(r\"([a-zA-Z])\\s+([{}])\".format(punctuations), r\"\\1\\2\", cleaned_text)\n",
    "    # Remove spaces between punctuation and another punctuation\n",
    "    cleaned_text = re.sub(r\"([{}])\\s+([{}])\".format(punctuations, punctuations), r\"\\1\\2\", cleaned_text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9420c997",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.472507Z",
     "iopub.status.busy": "2025-04-05T15:45:20.472295Z",
     "iopub.status.idle": "2025-04-05T15:45:20.484682Z",
     "shell.execute_reply": "2025-04-05T15:45:20.483993Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.472490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dual_embedding_features(df, modernbert_tokenizer, sbert_model, max_length=8192, sbert_batch_size=64):\n",
    "    \"\"\"\n",
    "    Prepare features for the dual embedding model, ensuring all tensors remain on CPU.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'Claim', 'Evidence', and label columns\n",
    "        modernbert_tokenizer: ModernBERT tokenizer\n",
    "        sbert_model: Sentence-BERT model\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "        sbert_batch_size: Batch size for SBERT encoding\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of feature tensors with input_ids, attention_mask, sbert_embeddings, and labels\n",
    "    \"\"\"\n",
    "    # Keep track of original SBERT device\n",
    "    original_device = next(sbert_model.parameters()).device\n",
    "    logger.info(f\"Original SBERT device: {original_device}\")\n",
    "\n",
    "    # Prepare inputs\n",
    "    texts_claim = df[\"Claim\"].tolist()\n",
    "    texts_evidence = df[\"Evidence\"].tolist()\n",
    "\n",
    "    # ModernBERT tokenization - keep on CPU\n",
    "    logger.info(\"Tokenizing inputs for ModernBERT...\")\n",
    "    modernbert_features = modernbert_tokenizer(\n",
    "        texts_claim,\n",
    "        texts_evidence,\n",
    "        padding=True,\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Compute SBERT embeddings on GPU, then move back to CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Computing SBERT embeddings on: {device}\")\n",
    "\n",
    "    # Temporarily move SBERT to computation device\n",
    "    sbert_model = sbert_model.to(device)\n",
    "\n",
    "    # Compute claim embeddings\n",
    "    logger.info(\"Computing SBERT embeddings for claims (batched)...\")\n",
    "    claim_embeddings = sbert_model.encode(\n",
    "        texts_claim,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=sbert_batch_size,\n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Move claim embeddings to CPU immediately\n",
    "    claim_embeddings = claim_embeddings.cpu()\n",
    "\n",
    "    # Compute evidence embeddings\n",
    "    logger.info(\"Computing SBERT embeddings for evidence (batched)...\")\n",
    "    evidence_embeddings = sbert_model.encode(\n",
    "        texts_evidence,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=sbert_batch_size,\n",
    "        show_progress_bar=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Move evidence embeddings to CPU immediately\n",
    "    evidence_embeddings = evidence_embeddings.cpu()\n",
    "\n",
    "    # Return SBERT to original device\n",
    "    sbert_model = sbert_model.to(original_device)\n",
    "\n",
    "    # Combine claim and evidence embeddings on CPU\n",
    "    logger.info(\"Combining embeddings...\")\n",
    "    combined_embeddings = []\n",
    "    for claim_emb, evid_emb in tqdm(zip(claim_embeddings, evidence_embeddings),\n",
    "                                  total=len(claim_embeddings),\n",
    "                                  desc=\"Combining embeddings\"):\n",
    "        # Use average of the claim and evidence embeddings\n",
    "        combined_emb = (claim_emb + evid_emb) / 2\n",
    "        combined_embeddings.append(combined_emb)\n",
    "\n",
    "    sbert_embeddings = torch.stack(combined_embeddings)\n",
    "\n",
    "    # Prepare labels\n",
    "    if \"label\" in df.columns:\n",
    "        label_col = \"label\"\n",
    "    elif \"labels\" in df.columns:\n",
    "        label_col = \"labels\"\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame must contain 'label' or 'labels' column\")\n",
    "\n",
    "    # Keep labels on CPU\n",
    "    labels = torch.tensor(df[label_col].values, dtype=torch.float)\n",
    "\n",
    "    # Final verification that all tensors are on CPU\n",
    "    logger.info(\"Verifying all tensors are on CPU...\")\n",
    "    for key, tensor in modernbert_features.items():\n",
    "        if tensor.is_cuda:\n",
    "            logger.warning(f\"{key} is on CUDA, moving to CPU\")\n",
    "            modernbert_features[key] = tensor.cpu()\n",
    "\n",
    "    if sbert_embeddings.is_cuda:\n",
    "        logger.warning(\"sbert_embeddings is on CUDA, moving to CPU\")\n",
    "        sbert_embeddings = sbert_embeddings.cpu()\n",
    "\n",
    "    if labels.is_cuda:\n",
    "        logger.warning(\"labels is on CUDA, moving to CPU\")\n",
    "        labels = labels.cpu()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": modernbert_features[\"input_ids\"],\n",
    "        \"attention_mask\": modernbert_features[\"attention_mask\"],\n",
    "        \"sbert_embeddings\": sbert_embeddings,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75534b7",
   "metadata": {},
   "source": [
    "## Model Loading Functions\n",
    "\n",
    "These functions load the dual embedding model from a local directory or from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7df5137b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.487118Z",
     "iopub.status.busy": "2025-04-05T15:45:20.486929Z",
     "iopub.status.idle": "2025-04-05T15:45:20.501520Z",
     "shell.execute_reply": "2025-04-05T15:45:20.500813Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.487102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_dual_embedding_model_from_hub(repo_id, device=None):\n",
    "    \"\"\"\n",
    "    Load a DualEmbeddingModel from Hugging Face Hub.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Hugging Face repository ID (e.g., \"username/model-name\")\n",
    "        device: Device to load the model to\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load tokenizer from Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained(repo_id, strip_accents=True)\n",
    "    \n",
    "    # Load SBERT model\n",
    "    sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Set up quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load base ModernBERT model\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        \"answerdotai/ModernBERT-base\",\n",
    "        quantization_config=quant_config,\n",
    "        device_map=device\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapters\n",
    "    peft_model = PeftModel.from_pretrained(base_model, repo_id, inference_mode=True)\n",
    "    \n",
    "    # Create DualEmbeddingModel\n",
    "    model = DualEmbeddingModel(peft_model)\n",
    "    \n",
    "    # Load classifier weights using huggingface_hub\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    \n",
    "    # Download classifier weights file\n",
    "    classifier_path = hf_hub_download(repo_id=repo_id, filename=\"classifier_weights.pt\")\n",
    "    classifier_weights = torch.load(classifier_path, map_location=device, weights_only=True)\n",
    "    model.classifier.load_state_dict(classifier_weights)\n",
    "    \n",
    "    # Load optimal threshold\n",
    "    threshold_path = hf_hub_download(repo_id=repo_id, filename=\"optimal_threshold.txt\")\n",
    "    with open(threshold_path, \"r\") as f:\n",
    "        threshold = float(f.read().strip())\n",
    "    \n",
    "    model.eval()\n",
    "    return model, tokenizer, sbert_model, threshold\n",
    "\n",
    "\n",
    "def load_dual_embedding_model_local(model_dir, device=None):\n",
    "    \"\"\"\n",
    "    Load a DualEmbeddingModel from a local directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Path to the local directory containing the model files\n",
    "        device: Device to load the model to (default: use CUDA if available)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"Loading model from {model_dir} to {device}\")\n",
    "    \n",
    "    # Load tokenizer from local directory\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, strip_accents=True)\n",
    "    \n",
    "    # Load SBERT model\n",
    "    sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Set up quantization config\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load base ModernBERT model\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        \"answerdotai/ModernBERT-base\",\n",
    "        quantization_config=quant_config,\n",
    "        device_map=device\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapters from local directory\n",
    "    peft_model = PeftModel.from_pretrained(base_model, model_dir, inference_mode=True)\n",
    "    \n",
    "    # Create DualEmbeddingModel\n",
    "    model = DualEmbeddingModel(peft_model)\n",
    "    \n",
    "    # Load classifier weights from local file\n",
    "    classifier_weights_path = os.path.join(model_dir, \"classifier_weights.pt\")\n",
    "    if os.path.exists(classifier_weights_path):\n",
    "        classifier_weights = torch.load(classifier_weights_path, map_location=device, weights_only=True)\n",
    "        model.classifier.load_state_dict(classifier_weights)\n",
    "        print(f\"Loaded classifier weights from {classifier_weights_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: Classifier weights file not found at {classifier_weights_path}\")\n",
    "    \n",
    "    # Load optimal threshold\n",
    "    threshold_path = os.path.join(model_dir, \"optimal_threshold.txt\")\n",
    "    threshold = 0.5  # Default threshold\n",
    "    if os.path.exists(threshold_path):\n",
    "        with open(threshold_path, \"r\") as f:\n",
    "            threshold = float(f.read().strip())\n",
    "        print(f\"Loaded optimal threshold: {threshold}\")\n",
    "    else:\n",
    "        print(f\"Warning: Threshold file not found at {threshold_path}, using default: {threshold}\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model, tokenizer, sbert_model, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e5f42b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.502703Z",
     "iopub.status.busy": "2025-04-05T15:45:20.502466Z",
     "iopub.status.idle": "2025-04-05T15:45:20.518706Z",
     "shell.execute_reply": "2025-04-05T15:45:20.517884Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.502656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for classification based on F1 score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): True labels.\n",
    "        y_pred (np.ndarray): Predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "        float: Optimal threshold.\n",
    "    \"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "\n",
    "    f1_scores = np.divide(\n",
    "        2 * (precisions * recalls),\n",
    "        (precisions + recalls),\n",
    "        out=np.zeros_like(precisions),\n",
    "        where=(precisions + recalls) > 0\n",
    "    )\n",
    "\n",
    "    best_idx = np.argmax(f1_scores[:-1])\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "\n",
    "    logger.info(f\"Best threshold: {best_threshold:.4f} with F1: {best_f1:.4f}\")\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49af4211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.519795Z",
     "iopub.status.busy": "2025-04-05T15:45:20.519563Z",
     "iopub.status.idle": "2025-04-05T15:45:20.528210Z",
     "shell.execute_reply": "2025-04-05T15:45:20.527388Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.519776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_best_threshold(model, dataloader, device=None):\n",
    "    \"\"\"\n",
    "    Calculate the best threshold for the model using the validation set,\n",
    "    converting logits to probabilities with sigmoid.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        dataloader: DataLoader for the validation set.\n",
    "        device: Device to run the model on. If None, will use CUDA if available.\n",
    "\n",
    "    Returns:\n",
    "        float: Best threshold based on F1 score.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_labels = []\n",
    "    all_probs = []  # We'll store probabilities, not logits\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Calculating best threshold\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            sbert_embeddings = batch[\"sbert_embeddings\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Get logits from model\n",
    "            logits = model(input_ids, attention_mask, sbert_embeddings)\n",
    "            \n",
    "            # Convert logits to probabilities using sigmoid\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all probabilities and labels\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # Find the optimal threshold based on F1 score\n",
    "    best_threshold = find_optimal_threshold(all_labels, all_probs)\n",
    "\n",
    "    logger.info(f\"Best threshold: {best_threshold:.4f}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62317569",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "These functions evaluate the model performance on a test dataset and calculate various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b118959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.529472Z",
     "iopub.status.busy": "2025-04-05T15:45:20.529188Z",
     "iopub.status.idle": "2025-04-05T15:45:20.546472Z",
     "shell.execute_reply": "2025-04-05T15:45:20.545736Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.529445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_dir, test_df, batch_size=16, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate the saved DualEmbeddingModel on a test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing the saved model and tokenizer.\n",
    "        test_df (pd.DataFrame): Test dataframe with 'Claim', 'Evidence', and 'label' columns.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        device (str, optional): Device to load the model to.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load the model\n",
    "    model, tokenizer, sbert_model, threshold = load_dual_embedding_model_from_hub(model_dir, device)\n",
    "    \n",
    "    # Explicitly move the entire model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Make sure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the test dataset\n",
    "    test_df[\"Claim\"] = test_df[\"Claim\"].apply(clean_text)\n",
    "    test_df[\"Evidence\"] = test_df[\"Evidence\"].apply(clean_text)\n",
    "\n",
    "    test_df[\"label\"] = test_df[\"label\"].astype(np.float16)\n",
    "\n",
    "    # Prepare the test dataset\n",
    "    test_features = prepare_dual_embedding_features(test_df, tokenizer, sbert_model)\n",
    "    test_dataset = DualEmbeddingDataset(test_features)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Perform evaluation\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\", disable=False):\n",
    "            # Explicitly move batch data to the same device as the model\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            sbert_embeddings = batch[\"sbert_embeddings\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, sbert_embeddings)\n",
    "            \n",
    "            # Move results back to CPU for metric calculation\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    \n",
    "    # Apply sigmoid to convert logits to probabilities\n",
    "    probabilities = torch.sigmoid(torch.tensor(all_logits)).numpy()\n",
    "    \n",
    "    # Make predictions using the optimal threshold\n",
    "    predictions = (probabilities > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, predictions, average='macro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_labels, predictions, average='weighted')\n",
    "    mcc = matthews_corrcoef(all_labels, predictions)\n",
    "\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, predictions, target_names=[\"0\", \"1\"], digits=6))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    cm = confusion_matrix(all_labels, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='crest', xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"])\n",
    "    plt.title(\"Confusion Matrix for ModernBERT + SBERT\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"Evaluation Results (threshold={threshold:.4f}):\")\n",
    "    print(f\"  Accuracy: {accuracy:.6f}\")\n",
    "    print(f\"  Precision (macro): {precision:.6f}\")\n",
    "    print(f\"  Recall (macro): {recall:.6f}\")\n",
    "    print(f\"  F1 Score (macro): {f1:.6f}\")\n",
    "    print(f\"  Precision (weighted): {precision_macro:.6f}\")\n",
    "    print(f\"  Recall (weighted): {recall_macro:.6f}\")\n",
    "    print(f\"  F1 Score (weighted): {f1_macro:.6f}\")\n",
    "    print(f\"  MCC: {mcc:.6f}\")\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        \"threshold\": threshold,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"precision_w\": precision_macro,\n",
    "        \"recall_w\": recall_macro,\n",
    "        \"f1_w\": f1_macro,\n",
    "        \"mcc\": mcc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d489aa7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.547347Z",
     "iopub.status.busy": "2025-04-05T15:45:20.547162Z",
     "iopub.status.idle": "2025-04-05T15:45:20.558762Z",
     "shell.execute_reply": "2025-04-05T15:45:20.558033Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.547330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_model(model_dir, test_df, batch_size=64, device=None):\n",
    "    \"\"\"\n",
    "    Use the saved DualEmbeddingModel to make predictions on a batch of claim-evidence pairs.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory containing the saved model and tokenizer.\n",
    "        test_df (pd.DataFrame): Test dataframe with 'Claim' and 'Evidence' columns.\n",
    "        batch_size (int): Batch size for prediction.\n",
    "        device (str, optional): Device to load the model to.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Original dataframe with 'prediction' and 'probability' columns added.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model, tokenizer, sbert_model, threshold = load_dual_embedding_model_from_hub(model_dir, device=device)\n",
    "\n",
    "    # Explicitly move the entire model to the specified device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Make sure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess the test dataset\n",
    "    test_df[\"Claim\"] = test_df[\"Claim\"].apply(clean_text)\n",
    "    test_df[\"Evidence\"] = test_df[\"Evidence\"].apply(clean_text)\n",
    "\n",
    "    test_df[\"label\"] = test_df[\"label\"].astype(np.float16)\n",
    "    \n",
    "    # Prepare the test dataset\n",
    "    test_features = prepare_dual_embedding_features(test_df, tokenizer, sbert_model)\n",
    "    test_dataset = DualEmbeddingDataset(test_features)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Perform predictions\n",
    "    all_logits = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\", disable=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            sbert_embeddings = batch[\"sbert_embeddings\"].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, sbert_embeddings)\n",
    "            all_logits.append(logits.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    \n",
    "    # Convert logits to probabilities with sigmoid\n",
    "    probabilities = torch.sigmoid(torch.tensor(all_logits)).numpy()\n",
    "    \n",
    "    # Make predictions using the optimal threshold\n",
    "    predictions = (probabilities > threshold).astype(int)\n",
    "    \n",
    "    # Add predictions and probabilities to the dataframe\n",
    "    result_df = test_df.copy()\n",
    "    result_df[\"prediction\"] = predictions\n",
    "    result_df[\"probability\"] = probabilities\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51ae7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.559637Z",
     "iopub.status.busy": "2025-04-05T15:45:20.559408Z",
     "iopub.status.idle": "2025-04-05T15:45:20.599921Z",
     "shell.execute_reply": "2025-04-05T15:45:20.599067Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.559619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = \"training_data/ED\"\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(DATASET_DIR, \"dev.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fabc1b",
   "metadata": {},
   "source": [
    "## Inference and Performance Analysis\n",
    "\n",
    "Here we load the model and run inference on the dev set to analyze its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b82680cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:45:20.600871Z",
     "iopub.status.busy": "2025-04-05T15:45:20.600656Z",
     "iopub.status.idle": "2025-04-05T15:47:15.664905Z",
     "shell.execute_reply": "2025-04-05T15:47:15.664178Z",
     "shell.execute_reply.started": "2025-04-05T15:45:20.600853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921f1ae4cd0c4effab8d2c052f36b50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a1994def5a41c7b74b61042824893e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining embeddings: 100%|| 5926/5926 [00:00<00:00, 83176.71it/s]\n",
      "Evaluating: 100%|| 93/93 [01:46<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.932713  0.889407  0.910546      4286\n",
      "           1   0.742251  0.832317  0.784708      1640\n",
      "\n",
      "    accuracy                       0.873608      5926\n",
      "   macro avg   0.837482  0.860862  0.847627      5926\n",
      "weighted avg   0.880004  0.873608  0.875721      5926\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results (threshold=0.5433):\n",
      "  Accuracy: 0.873608\n",
      "  Precision (macro): 0.837482\n",
      "  Recall (macro): 0.860862\n",
      "  F1 Score (macro): 0.847627\n",
      "  Precision (weighted): 0.880004\n",
      "  Recall (weighted): 0.873608\n",
      "  F1 Score (weighted): 0.875721\n",
      "  MCC: 0.697953\n"
     ]
    }
   ],
   "source": [
    "# Load the model directory\n",
    "model_dir = \"ddosdub/DualEncoderModernBERT\"\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_model(model_dir, test_df, batch_size=64, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23748aec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:47:15.666961Z",
     "iopub.status.busy": "2025-04-05T15:47:15.666348Z",
     "iopub.status.idle": "2025-04-05T15:49:10.893626Z",
     "shell.execute_reply": "2025-04-05T15:49:10.892725Z",
     "shell.execute_reply.started": "2025-04-05T15:47:15.666936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd1bff88f6642699ddd84105b731821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacb10534e4b46d6bc4d75476e97eab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining embeddings: 100%|| 5926/5926 [00:00<00:00, 101867.83it/s]\n",
      "Predicting: 100%|| 93/93 [01:46<00:00,  1.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Democracy should be done away with.</td>\n",
       "      <td>Amartya Sen, an Indian economist and Nobel lau...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polygamy should be made legal.</td>\n",
       "      <td>The Supreme Court's unanimous decision in Reyn...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hunting should be banned</td>\n",
       "      <td>In total it is estimated that Ceausescu receiv...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Television should be given up.</td>\n",
       "      <td>Barbera mentioned that they had to either adju...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abortions ought to be prohibited.</td>\n",
       "      <td>According to a poll conducted by Angus Reid St...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Claim  \\\n",
       "0  Democracy should be done away with.   \n",
       "1       Polygamy should be made legal.   \n",
       "2             Hunting should be banned   \n",
       "3       Television should be given up.   \n",
       "4    Abortions ought to be prohibited.   \n",
       "\n",
       "                                            Evidence  label  prediction  \\\n",
       "0  Amartya Sen, an Indian economist and Nobel lau...    0.0           0   \n",
       "1  The Supreme Court's unanimous decision in Reyn...    1.0           1   \n",
       "2  In total it is estimated that Ceausescu receiv...    0.0           0   \n",
       "3  Barbera mentioned that they had to either adju...    0.0           0   \n",
       "4  According to a poll conducted by Angus Reid St...    1.0           1   \n",
       "\n",
       "   probability  \n",
       "0     0.467568  \n",
       "1     0.997100  \n",
       "2     0.001340  \n",
       "3     0.003276  \n",
       "4     0.996988  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df = predict_model(model_dir, test_df, batch_size=64, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3645477a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T15:49:10.894797Z",
     "iopub.status.busy": "2025-04-05T15:49:10.894546Z",
     "iopub.status.idle": "2025-04-05T15:49:10.910041Z",
     "shell.execute_reply": "2025-04-05T15:49:10.909346Z",
     "shell.execute_reply.started": "2025-04-05T15:49:10.894777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get prediction column from the predict_df\n",
    "predictions = predict_df[\"prediction\"]\n",
    "\n",
    "# convert to csv\n",
    "predictions.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "def create_file_for_submission(predictions: pd.DataFrame, ext: str = \"predict\"):\n",
    "    predictions.to_csv(f\"predictions.csv.{ext}\", index=False)\n",
    "\n",
    "    # zip the file\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"predictions.zip\", \"w\") as zf:\n",
    "        zf.write(f\"predictions.csv.{ext}\")\n",
    "\n",
    "create_file_for_submission(pd.read_csv(\"predictions.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6893622,
     "sourceId": 11063404,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
